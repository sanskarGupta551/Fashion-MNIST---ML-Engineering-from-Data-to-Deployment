{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374eae5c",
   "metadata": {},
   "source": [
    "# **Fashion MNIST: Ingest Raw Data**\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894e7db",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80607399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 14:52:35.816835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745765555.827071   71150 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745765555.830115   71150 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745765555.838817   71150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745765555.838833   71150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745765555.838835   71150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745765555.838836   71150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-27 14:52:35.841849: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f516f3",
   "metadata": {},
   "source": [
    "### **Download and Prepare Data for Multiple Formats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a804de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local directory structure\n",
    "os.makedirs('./fashion_mnist_data', exist_ok=True)\n",
    "os.makedirs('./fashion_mnist_data/custom_jobs', exist_ok=True)\n",
    "os.makedirs('./fashion_mnist_data/vertex_datasets', exist_ok=True)\n",
    "os.makedirs('./fashion_mnist_data/vertex_datasets/train', exist_ok=True)\n",
    "os.makedirs('./fashion_mnist_data/vertex_datasets/test', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b77a73aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28)\n",
      "Test data shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load Fashion MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Define class names\n",
    "class_names = ['T-shirt_top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle_boot']\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4bd0b",
   "metadata": {},
   "source": [
    "### **Save in Multiple Formats**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c8ba4",
   "metadata": {},
   "source": [
    "##### **Format 1: NumPy Arrays (Best for Vertex AI Custom Jobs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2601902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy format for Custom Jobs saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Save as NumPy arrays for Custom Jobs\n",
    "# This format is most efficient for custom training scripts\n",
    "np.savez_compressed(\n",
    "    './fashion_mnist_data/custom_jobs/fashion_mnist.npz',\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Save class names\n",
    "with open('./fashion_mnist_data/custom_jobs/class_names.json', 'w') as f:\n",
    "    json.dump(class_names, f)\n",
    "\n",
    "print(\"NumPy format for Custom Jobs saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf46953",
   "metadata": {},
   "source": [
    "##### **Format 2: Images with CSV (Best for Vertex AI Datasets)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e684e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save images and create CSV for Vertex AI Datasets\n",
    "def create_vertex_dataset_format(images, labels, split='train'):\n",
    "    csv_data = []\n",
    "    \n",
    "    # Create class folders\n",
    "    for class_name in class_names:\n",
    "        os.makedirs(f'./fashion_mnist_data/vertex_datasets/{split}/{class_name}', exist_ok=True)\n",
    "    \n",
    "    for idx, (image, label) in enumerate(zip(images, labels)):\n",
    "        class_name = class_names[label]\n",
    "        image_filename = f\"{split}_{idx:05d}.jpg\"\n",
    "        \n",
    "        # Save as JPEG (better compression for storage)\n",
    "        local_path = f'./fashion_mnist_data/vertex_datasets/{split}/{class_name}/{image_filename}'\n",
    "        Image.fromarray(image).convert('L').save(local_path, 'JPEG', quality=95)\n",
    "        \n",
    "        # GCS path format for Vertex AI Datasets\n",
    "        gcs_path = f'gs://fashion-mnist-datasets/vertex_datasets/{split}/{class_name}/{image_filename}'\n",
    "        \n",
    "        # Add to CSV data\n",
    "        csv_data.append({\n",
    "            'gcs_path': gcs_path,\n",
    "            'label': class_name\n",
    "        })\n",
    "        \n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Processed {idx} {split} images...\")\n",
    "    \n",
    "    return pd.DataFrame(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d4f6b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 train images...\n",
      "Processed 1000 train images...\n",
      "Processed 2000 train images...\n",
      "Processed 3000 train images...\n",
      "Processed 4000 train images...\n",
      "Processed 5000 train images...\n",
      "Processed 6000 train images...\n",
      "Processed 7000 train images...\n",
      "Processed 8000 train images...\n",
      "Processed 9000 train images...\n",
      "Processed 10000 train images...\n",
      "Processed 11000 train images...\n",
      "Processed 12000 train images...\n",
      "Processed 13000 train images...\n",
      "Processed 14000 train images...\n",
      "Processed 15000 train images...\n",
      "Processed 16000 train images...\n",
      "Processed 17000 train images...\n",
      "Processed 18000 train images...\n",
      "Processed 19000 train images...\n",
      "Processed 20000 train images...\n",
      "Processed 21000 train images...\n",
      "Processed 22000 train images...\n",
      "Processed 23000 train images...\n",
      "Processed 24000 train images...\n",
      "Processed 25000 train images...\n",
      "Processed 26000 train images...\n",
      "Processed 27000 train images...\n",
      "Processed 28000 train images...\n",
      "Processed 29000 train images...\n",
      "Processed 30000 train images...\n",
      "Processed 31000 train images...\n",
      "Processed 32000 train images...\n",
      "Processed 33000 train images...\n",
      "Processed 34000 train images...\n",
      "Processed 35000 train images...\n",
      "Processed 36000 train images...\n",
      "Processed 37000 train images...\n",
      "Processed 38000 train images...\n",
      "Processed 39000 train images...\n",
      "Processed 40000 train images...\n",
      "Processed 41000 train images...\n",
      "Processed 42000 train images...\n",
      "Processed 43000 train images...\n",
      "Processed 44000 train images...\n",
      "Processed 45000 train images...\n",
      "Processed 46000 train images...\n",
      "Processed 47000 train images...\n",
      "Processed 48000 train images...\n",
      "Processed 49000 train images...\n",
      "Processed 50000 train images...\n",
      "Processed 51000 train images...\n",
      "Processed 52000 train images...\n",
      "Processed 53000 train images...\n",
      "Processed 54000 train images...\n",
      "Processed 55000 train images...\n",
      "Processed 56000 train images...\n",
      "Processed 57000 train images...\n",
      "Processed 58000 train images...\n",
      "Processed 59000 train images...\n",
      "Processed 0 test images...\n",
      "Processed 1000 test images...\n",
      "Processed 2000 test images...\n",
      "Processed 3000 test images...\n",
      "Processed 4000 test images...\n",
      "Processed 5000 test images...\n",
      "Processed 6000 test images...\n",
      "Processed 7000 test images...\n",
      "Processed 8000 test images...\n",
      "Processed 9000 test images...\n",
      "Vertex AI Datasets format saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_df = create_vertex_dataset_format(X_train, y_train, 'train')\n",
    "test_df = create_vertex_dataset_format(X_test, y_test, 'test')\n",
    "\n",
    "# Save CSV for Vertex AI import\n",
    "train_df.to_csv('./fashion_mnist_data/vertex_datasets/train.csv', index=False, header=False)\n",
    "test_df.to_csv('./fashion_mnist_data/vertex_datasets/test.csv', index=False, header=False)\n",
    "\n",
    "# Create combined CSV if needed\n",
    "combined_df = pd.concat([train_df, test_df])\n",
    "combined_df.to_csv('./fashion_mnist_data/vertex_datasets/all_data.csv', index=False, header=False)\n",
    "\n",
    "print(\"Vertex AI Datasets format saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95925712",
   "metadata": {},
   "source": [
    "##### **Create README and Manifest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "162b8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create README file\n",
    "readme_content = f\"\"\"# Fashion MNIST Dataset for Vertex AI\n",
    "\n",
    "This directory contains the Fashion MNIST dataset optimized for Google Cloud Vertex AI.\n",
    "\n",
    "## Directory Structure:\n",
    "- `/custom_jobs`: NumPy compressed arrays for Vertex AI Custom Training Jobs\n",
    "  - `fashion_mnist.npz`: Contains X_train, y_train, X_test, y_test\n",
    "  - `class_names.json`: List of class names\n",
    "  \n",
    "- `/vertex_datasets`: Images with CSV for Vertex AI Datasets\n",
    "  - `/train`: Training images organized by class\n",
    "  - `/test`: Test images organized by class\n",
    "  - `train.csv`: CSV for training data import\n",
    "  - `test.csv`: CSV for test data import\n",
    "  - `all_data.csv`: Combined CSV for full dataset import\n",
    "\n",
    "## Dataset Details:\n",
    "- Training samples: {len(X_train)}\n",
    "- Test samples: {len(X_test)}\n",
    "- Image dimensions: 28x28 grayscale\n",
    "- Classes: {', '.join(class_names)}\n",
    "\n",
    "## Usage:\n",
    "1. For Vertex AI Custom Training: Load the NPZ file\n",
    "   ```python\n",
    "   data = np.load('fashion_mnist.npz')\n",
    "   X_train, y_train = data['X_train'], data['y_train']\n",
    "   X_test, y_test = data['X_test'], data['y_test']\n",
    "2. For Vertex AI Datasets: Upload the CSV files to create Image Datasets\n",
    "\n",
    "Use all_data.csv for the complete dataset\n",
    "Use separate train/test CSVs for split datasets\n",
    "\n",
    "Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "with open('./fashion_mnist_data/README.md', 'w') as f:\n",
    "  f.write(readme_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82ef3dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README and manifest created successfully\n"
     ]
    }
   ],
   "source": [
    "manifest = {\n",
    "'dataset_name': 'Fashion MNIST',\n",
    "'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "'formats': {\n",
    "'custom_jobs': {\n",
    "'type': 'numpy_compressed',\n",
    "'files': ['fashion_mnist.npz', 'class_names.json']\n",
    "},\n",
    "'vertex_datasets': {\n",
    "'type': 'images_with_csv',\n",
    "'files': ['train.csv', 'test.csv', 'all_data.csv'],\n",
    "'image_format': 'JPEG'\n",
    "}\n",
    "},\n",
    "'statistics': {\n",
    "'train_samples': len(X_train),\n",
    "'test_samples': len(X_test),\n",
    "'image_shape': list(X_train[0].shape),\n",
    "'classes': class_names\n",
    "},\n",
    "'gcs_bucket': 'fashion-mnist-datasets'\n",
    "}\n",
    "with open('./fashion_mnist_data/manifest.json', 'w') as f:\n",
    "    json.dump(manifest, f, indent=4)\n",
    "\n",
    "print(\"README and manifest created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480075e",
   "metadata": {},
   "source": [
    "### **Verify Local Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97e90961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying local files...\n",
      "✅ ./fashion_mnist_data/README.md (0.00 MB)\n",
      "✅ ./fashion_mnist_data/manifest.json (0.00 MB)\n",
      "✅ ./fashion_mnist_data/custom_jobs/fashion_mnist.npz (29.44 MB)\n",
      "✅ ./fashion_mnist_data/custom_jobs/class_names.json (0.00 MB)\n",
      "✅ ./fashion_mnist_data/vertex_datasets/train.csv (4.65 MB)\n",
      "✅ ./fashion_mnist_data/vertex_datasets/test.csv (0.76 MB)\n",
      "✅ ./fashion_mnist_data/vertex_datasets/all_data.csv (5.40 MB)\n",
      "\n",
      "Image counts:\n",
      "Training images: 60000\n",
      "Test images: 10000\n",
      "\n",
      "NPZ file contents:\n",
      "- X_train: shape (60000, 28, 28)\n",
      "- y_train: shape (60000,)\n",
      "- X_test: shape (10000, 28, 28)\n",
      "- y_test: shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Verify local files\n",
    "def verify_local_files():\n",
    "    print(\"\\nVerifying local files...\")\n",
    "    \n",
    "    files_to_check = [\n",
    "        './fashion_mnist_data/README.md',\n",
    "        './fashion_mnist_data/manifest.json',\n",
    "        './fashion_mnist_data/custom_jobs/fashion_mnist.npz',\n",
    "        './fashion_mnist_data/custom_jobs/class_names.json',\n",
    "        './fashion_mnist_data/vertex_datasets/train.csv',\n",
    "        './fashion_mnist_data/vertex_datasets/test.csv',\n",
    "        './fashion_mnist_data/vertex_datasets/all_data.csv'\n",
    "    ]\n",
    "    \n",
    "    for file_path in files_to_check:\n",
    "        if os.path.exists(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"✅ {file_path} ({size_mb:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"❌ {file_path} missing\")\n",
    "    \n",
    "    # Count images\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        train_path = f'./fashion_mnist_data/vertex_datasets/train/{class_name}'\n",
    "        test_path = f'./fashion_mnist_data/vertex_datasets/test/{class_name}'\n",
    "        \n",
    "        if os.path.exists(train_path):\n",
    "            train_count += len([f for f in os.listdir(train_path) if f.endswith('.jpg')])\n",
    "        if os.path.exists(test_path):\n",
    "            test_count += len([f for f in os.listdir(test_path) if f.endswith('.jpg')])\n",
    "    \n",
    "    print(f\"\\nImage counts:\")\n",
    "    print(f\"Training images: {train_count}\")\n",
    "    print(f\"Test images: {test_count}\")\n",
    "    \n",
    "    # Verify NPZ file\n",
    "    try:\n",
    "        data = np.load('./fashion_mnist_data/custom_jobs/fashion_mnist.npz')\n",
    "        print(f\"\\nNPZ file contents:\")\n",
    "        for key in data.files:\n",
    "            print(f\"- {key}: shape {data[key].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading NPZ file: {e}\")\n",
    "\n",
    "verify_local_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9097228e",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4141589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow-GPU)",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
