{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7acdfec1-57a6-484e-97aa-325bd025e421",
   "metadata": {},
   "source": [
    "# **Fashion MNIST: Data Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b6b7b",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "This notebook orchestrates the execution of our production-grade data normalization pipeline for the Fashion MNIST dataset on Google Cloud Platform. Instead of performing normalization interactively, we implement a cloud-native, reproducible approach that:\n",
    "\n",
    "1. Downloads our pre-built `normalization script` from Cloud Storage\n",
    "2. Executes the script against our raw dataset in GCS\n",
    "3. Outputs normalized data optimized for machine learning workflows\n",
    "\n",
    "The normalization process follows the recommendations from our data analysis phase, applying `Min-Max [0,1] normalization` by converting images from uint8 to float32 and scaling by 255.0.\n",
    "\n",
    "This engineering approach follows ML production best practices by:\n",
    "- Separating preprocessing logic from exploration notebooks\n",
    "- Creating reusable, modular normalization components\n",
    "- Enabling reproducible preprocessing across environments\n",
    "- Establishing proper error handling and logging\n",
    "- Generating comprehensive documentation\n",
    "\n",
    "The main Python module (`main.py`) implements a robust normalization pipeline with both HTTP and Cloud Storage triggers, allowing for flexible integration with various workflow orchestration systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7ace9",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Cloud Environment Setup**\n",
    "\n",
    "Above, we've downloaded the necessary components for our normalization pipeline:\n",
    "\n",
    "1. `main.py` - The core normalization module containing:\n",
    "   - Cloud Function implementations for both HTTP and GCS triggers\n",
    "   - Robust error handling and comprehensive logging\n",
    "   - GCS download and upload utilities\n",
    "   - Normalization logic that converts images to `float32` and scales to `[0,1]`\n",
    "   - Documentation generation for processed datasets\n",
    "\n",
    "2. `requirements.txt` - Dependencies needed for the normalization process:\n",
    "   - `numpy==1.26.0` for numerical operations\n",
    "   - `google-cloud-storage==2.12.0` for GCS operations\n",
    "   - `functions-framework==3.5.0` for Cloud Functions local testing\n",
    "\n",
    "These files are retrieved from the `fashion-mnist-dev` GCS bucket where we maintain our data engineering components. This approach ensures version control of our preprocessing code and enables consistent preprocessing across different environments.\n",
    "\n",
    "Next, we'll install the required dependencies to ensure our environment has all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6fb2f8a-bc64-4e2e-bd40-bbcbf4de521f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files downloaded successfully to ~/data_normalizer/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "# Create a local directory for your files\n",
    "os.makedirs(os.path.expanduser(\"~/data_normalizer\"), exist_ok=True)\n",
    "\n",
    "# Download the files from GCS\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(\"fashion-mnist-dev\")\n",
    "\n",
    "# Download main.py\n",
    "blob = bucket.blob(\"data-normalizer/main.py\")\n",
    "blob.download_to_filename(os.path.expanduser(\"~/data_normalizer/main.py\"))\n",
    "\n",
    "# Download requirements.txt\n",
    "blob = bucket.blob(\"data-normalizer/requirements.txt\")\n",
    "blob.download_to_filename(os.path.expanduser(\"~/data_normalizer/requirements.txt\"))\n",
    "\n",
    "print(\"Files downloaded successfully to ~/data_normalizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1d34ec1-3527-4ff0-9c7f-71c9cc0fa610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.0 in /opt/conda/lib/python3.10/site-packages (from -r /home/jupyter/data_normalizer/requirements.txt (line 1)) (1.26.0)\n",
      "Requirement already satisfied: google-cloud-storage==2.12.0 in /opt/conda/lib/python3.10/site-packages (from -r /home/jupyter/data_normalizer/requirements.txt (line 2)) (2.12.0)\n",
      "Requirement already satisfied: functions-framework==3.5.0 in /opt/conda/lib/python3.10/site-packages (from -r /home/jupyter/data_normalizer/requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.23.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (2.38.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (2.24.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (2.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (1.7.1)\n",
      "Requirement already satisfied: flask<4.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from functions-framework==3.5.0->-r /home/jupyter/data_normalizer/requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: click<9.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from functions-framework==3.5.0->-r /home/jupyter/data_normalizer/requirements.txt (line 3)) (8.1.8)\n",
      "Requirement already satisfied: watchdog>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from functions-framework==3.5.0->-r /home/jupyter/data_normalizer/requirements.txt (line 3)) (6.0.0)\n",
      "Requirement already satisfied: cloudevents<2.0.0,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from functions-framework==3.5.0->-r /home/jupyter/data_normalizer/requirements.txt (line 3)) (1.11.0)\n",
      "Requirement already satisfied: gunicorn>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from functions-framework==3.5.0->-r /home/jupyter/data_normalizer/requirements.txt (line 3)) (23.0.0)\n",
      "Requirement already satisfied: deprecation<3.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from cloudevents<2.0.0,>=1.2.0->functions-framework==3.5.0->-r /home/jupyter/data_normalizer/requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /opt/conda/lib/python3.10/site-packages (from flask<4.0,>=1.0->functions-framework==3.5.0->-r /home/jupyter/data_normalizer/requirements.txt (line 3)) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from flask<4.0,>=1.0->functions-framework==3.5.0->-r /home/jupyter/data_normalizer/requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /opt/conda/lib/python3.10/site-packages (from flask<4.0,>=1.0->functions-framework==3.5.0->-r /home/jupyter/data_normalizer/requirements.txt (line 3)) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.9 in /opt/conda/lib/python3.10/site-packages (from flask<4.0,>=1.0->functions-framework==3.5.0->-r /home/jupyter/data_normalizer/requirements.txt (line 3)) (1.9.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (1.69.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (3.20.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (1.26.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (4.9)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gunicorn>=19.2.0->functions-framework==3.5.0->-r /home/jupyter/data_normalizer/requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask<4.0,>=1.0->functions-framework==3.5.0->-r /home/jupyter/data_normalizer/requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.12.0->-r /home/jupyter/data_normalizer/requirements.txt (line 2)) (0.6.1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pip', 'install', '-r', '/home/jupyter/data_normalizer/requirements.txt'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"pip\", \"install\", \"-r\", os.path.expanduser(\"~/data_normalizer/requirements.txt\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd2bf6",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Configuration**\n",
    "\n",
    "After installing dependencies, we'll create a simple runner script that:\n",
    "\n",
    "1. Imports the `normalize_fashion_mnist` function from our module\n",
    "2. Configures input and output paths in Google Cloud Storage\n",
    "3. Executes the normalization process\n",
    "4. Reports the results in JSON format\n",
    "\n",
    "The input path points to our raw dataset stored as `fashion_mnist.npz` in the `custom_jobs` directory, and the output will be saved to the `custom_jobs_normalized` directory. This mirrors the pattern implemented in the Cloud Function that automatically transforms input directories to normalized versions.\n",
    "\n",
    "The normalization process will:\n",
    "- Load the raw Fashion MNIST data from the NPZ file\n",
    "- Apply `Min-Max [0,1] normalization` through the `normalize_dataset` function\n",
    "- Create a detailed README.md with usage instructions\n",
    "- Copy the class_names.json file if present\n",
    "- Save the normalized data in compressed NPZ format with the original split structure preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2094f533-2f14-4743-89fd-567d84284610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runner script created at ~/data_normalizer/run.py\n"
     ]
    }
   ],
   "source": [
    "# Create a simple runner script\n",
    "with open(os.path.expanduser(\"~/data_normalizer/run.py\"), \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "import sys\n",
    "sys.path.append(\"/home/jupyter/data_normalizer\")\n",
    "\n",
    "from main import normalize_fashion_mnist\n",
    "import json\n",
    "\n",
    "# Define your input and output paths\n",
    "input_path = \"gs://fashion-mnist-datasets/custom_jobs/fashion_mnist.npz\"\n",
    "output_path = \"gs://fashion-mnist-datasets/custom_jobs_normalized/\"\n",
    "\n",
    "# Run the normalization function\n",
    "result = normalize_fashion_mnist(input_path, output_path)\n",
    "print(f\"Normalization result: {json.dumps(result, indent=2)}\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"Runner script created at ~/data_normalizer/run.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56359842",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Execution**\n",
    "\n",
    "Now we'll execute the normalization process by running our script. The script will:\n",
    "\n",
    "1. Download the raw Fashion MNIST dataset from Google Cloud Storage\n",
    "2. Load the compressed NPZ file containing training and test data\n",
    "3. Convert images from `uint8` (0-255) to `float32` (0.0-1.0) using the carefully calibrated normalization function\n",
    "4. Preserve all dataset splits (training, validation, test) while maintaining label consistency\n",
    "5. Create a comprehensive README.md with usage examples and preprocessing details\n",
    "6. Save the normalized data in compressed NPZ format for efficient storage and loading\n",
    "7. Upload all artifacts back to Google Cloud Storage\n",
    "\n",
    "This execution demonstrates the standalone functionality of our normalization module, which could alternatively be deployed as a Cloud Function triggered by storage events or HTTP requests as implemented in the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53756ae-989d-476b-afaf-c7a5d3b6cb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 19:17:13,467 - main - INFO - Downloading gs://fashion-mnist-datasets/custom_jobs/fashion_mnist.npz to /var/tmp/tmpcrtwd2tx/fashion_mnist.npz\n",
      "2025-04-28 19:17:14,053 - main - INFO - Download complete\n",
      "2025-04-28 19:17:14,053 - main - INFO - Loading dataset from /var/tmp/tmpcrtwd2tx/fashion_mnist.npz\n",
      "2025-04-28 19:17:14,417 - main - INFO - Dataset loaded with keys: dict_keys(['X_train', 'y_train', 'X_test', 'y_test'])\n",
      "2025-04-28 19:17:14,417 - main - INFO - Normalizing dataset\n",
      "2025-04-28 19:17:14,512 - main - INFO - Normalized X_train: shape=(60000, 28, 28), dtype=float32\n",
      "2025-04-28 19:17:14,513 - main - INFO - Kept y_train unchanged: shape=(60000,), dtype=uint8\n",
      "2025-04-28 19:17:14,530 - main - INFO - Normalized X_test: shape=(10000, 28, 28), dtype=float32\n",
      "2025-04-28 19:17:14,530 - main - INFO - Kept y_test unchanged: shape=(10000,), dtype=uint8\n",
      "2025-04-28 19:17:14,530 - main - INFO - Saving normalized dataset to /var/tmp/tmpcrtwd2tx/fashion_mnist_normalized.npz\n",
      "2025-04-28 19:17:23,107 - main - INFO - Uploading /var/tmp/tmpcrtwd2tx/fashion_mnist_normalized.npz to gs://fashion-mnist-datasets/custom_jobs_normalized/fashion_mnist_normalized.npz\n",
      "2025-04-28 19:17:23,607 - main - INFO - Upload complete\n",
      "2025-04-28 19:17:23,610 - main - INFO - Downloading gs://fashion-mnist-datasets/custom_jobs/class_names.json to /var/tmp/tmpcrtwd2tx/class_names.json\n",
      "2025-04-28 19:17:23,692 - main - INFO - Download complete\n",
      "2025-04-28 19:17:23,696 - main - INFO - Uploading /var/tmp/tmpcrtwd2tx/class_names.json to gs://fashion-mnist-datasets/custom_jobs_normalized/class_names.json\n",
      "2025-04-28 19:17:23,800 - main - INFO - Upload complete\n",
      "2025-04-28 19:17:23,800 - main - INFO - Copied class_names.json to gs://fashion-mnist-datasets/custom_jobs_normalized/class_names.json\n",
      "2025-04-28 19:17:23,803 - main - INFO - Uploading /var/tmp/tmpcrtwd2tx/README.md to gs://fashion-mnist-datasets/custom_jobs_normalized/README.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization result: {\n",
      "  \"status\": \"success\",\n",
      "  \"input_path\": \"gs://fashion-mnist-datasets/custom_jobs/fashion_mnist.npz\",\n",
      "  \"output_path\": \"gs://fashion-mnist-datasets/custom_jobs_normalized/\",\n",
      "  \"normalized_file\": \"gs://fashion-mnist-datasets/custom_jobs_normalized/fashion_mnist_normalized.npz\",\n",
      "  \"timestamp\": \"2025-04-28T19:17:23.926234\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 19:17:23,926 - main - INFO - Upload complete\n",
      "2025-04-28 19:17:23,926 - main - INFO - Created and uploaded README.md to gs://fashion-mnist-datasets/custom_jobs_normalized/README.md\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '/home/jupyter/data_normalizer/run.py'], returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"python\", os.path.expanduser(\"~/data_normalizer/run.py\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6add4ea",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Validation**\n",
    "\n",
    "After running the normalization process, we'll verify the results by:\n",
    "\n",
    "1. Listing the contents of the output directory in Google Cloud Storage\n",
    "2. Confirming the creation of:\n",
    "   - `fashion_mnist_normalized.npz` - The normalized dataset file\n",
    "   - `class_names.json` - The class mapping information\n",
    "   - `README.md` - Documentation with usage instructions\n",
    "\n",
    "We expect to see these files with appropriate timestamps and sizes. The normalized NPZ file should be similar in size to the original, as the compression efficiency is typically maintained despite the data type conversion from uint8 to float32.\n",
    "\n",
    "This validation step ensures our preprocessing pipeline executed correctly before proceeding to model training. In a production environment, this would be supplemented with additional data quality checks and automated testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea12f003-dd36-44bf-86a3-fa0ca8ac6dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1058  2025-04-28T19:17:23Z  gs://fashion-mnist-datasets/custom_jobs_normalized/README.md\n",
      "       106  2025-04-28T19:17:23Z  gs://fashion-mnist-datasets/custom_jobs_normalized/class_names.json\n",
      "  45395155  2025-04-28T19:17:23Z  gs://fashion-mnist-datasets/custom_jobs_normalized/fashion_mnist_normalized.npz\n",
      "TOTAL: 3 objects, 45396319 bytes (43.29 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['gsutil', 'ls', '-l', 'gs://fashion-mnist-datasets/custom_jobs_normalized/'], returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"gsutil\", \"ls\", \"-l\", \"gs://fashion-mnist-datasets/custom_jobs_normalized/\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6721575",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Cleanup**\n",
    "\n",
    "As a final step, we'll clean up resources by removing the Cloud Function used for normalization. While our current execution used the local Python module directly, this function was previously deployed to automatically process new datasets uploaded to our bucket.\n",
    "\n",
    "This cleanup is an important practice in cloud environments to:\n",
    "\n",
    "1. Avoid unnecessary ongoing resource costs\n",
    "2. Maintain a clean cloud project environment\n",
    "3. Prevent potential naming conflicts in future deployments\n",
    "\n",
    "The Google Cloud SDK command `gcloud functions delete` handles the removal of our serverless function resources. In a production environment, this type of cleanup would typically be managed by Infrastructure as Code (IaC) tools or CI/CD pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56f44964-2886-47b7-9dd9-2747c71b9940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: Preparing function...\n",
      "......done.\n",
      "Deleting function...\n",
      "[Service]..........................done\n",
      "[Artifact Registry]done\n",
      "Done.\n",
      "Deleted [projects/fashion-mnist-gcp/locations/us-central1/functions/fashion-mnist-normalizer].\n",
      "\n",
      "Return code: 0\n",
      "Cloud Function successfully deleted!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Delete the Cloud Function\n",
    "result = subprocess.run(\n",
    "    [\"gcloud\", \"functions\", \"delete\", \"fashion-mnist-normalizer\", \n",
    "     \"--region=us-central1\", \"--quiet\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Print the output\n",
    "print(\"STDOUT:\", result.stdout)\n",
    "print(\"STDERR:\", result.stderr)\n",
    "print(f\"Return code: {result.returncode}\")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"Cloud Function successfully deleted!\")\n",
    "else:\n",
    "    print(\"Error deleting Cloud Function. See error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79b188f",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "\n",
    "1. Implemented a scalable, reproducible data normalization pipeline leveraging Google Cloud Storage\n",
    "2. Processed the Fashion MNIST dataset with industry-standard `Min-Max [0,1] normalization`\n",
    "3. Generated normalized floating-point data optimized for neural network training\n",
    "4. Created comprehensive documentation for downstream consumers of the dataset\n",
    "5. Demonstrated proper cloud resource management and cleanup\n",
    "\n",
    "Our approach leverages several production ML engineering best practices:\n",
    "\n",
    "- **Modularity**: The normalization logic is encapsulated in a standalone Python module\n",
    "- **Flexibility**: The implementation supports both interactive execution and serverless deployment\n",
    "- **Reproducibility**: The process applies consistent normalization with fixed random seeds\n",
    "- **Documentation**: Automatic README generation ensures consumers understand the data format\n",
    "- **Error handling**: Robust exception handling and logging throughout the pipeline\n",
    "\n",
    "The normalized dataset is now ready for model training in subsequent notebooks and Vertex AI custom training jobs. The preprocessing decisions implemented here align with our findings from the data analysis phase, specifically the recommendation to use Min-Max normalization for optimal model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe015b28-c3d8-4637-a611-036a04271df3",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d174555-ffcd-4008-9865-2108b2fd9635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
