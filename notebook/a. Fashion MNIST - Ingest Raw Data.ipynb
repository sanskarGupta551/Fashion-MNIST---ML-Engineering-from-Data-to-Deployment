{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374eae5c",
   "metadata": {},
   "source": [
    "# **Fashion MNIST: Ingest Raw Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce60a38",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "This notebook demonstrates the data ingestion process for the `Fashion MNIST` dataset, preparing it for both custom training jobs and AutoML training in Google Cloud Platform's `Vertex AI`. The process includes:\n",
    "\n",
    "1. Downloading the raw `Fashion MNIST` dataset from **tensorflow**\n",
    "2. Processing data into multiple formats optimized for different GCP services\n",
    "3. Creating proper directory structures and documentation\n",
    "4. Generating metadata and manifest files\n",
    "5. Verifying data integrity before cloud upload\n",
    "\n",
    "The `Fashion MNIST` dataset consists of `70,000 grayscale` images (`28×28` pixels) of clothing items across 10 categories. We'll prepare this data in formats specifically optimized for GCP's ML workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bc05f",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Dataset Overview**\n",
    "\n",
    "`Fashion MNIST` is a dataset of Zalando's article images consisting of:\n",
    "- **60,000** training examples\n",
    "- **10,000** test examples\n",
    "- **10** classes of fashion items\n",
    "- **28×28** `grayscale` images\n",
    "\n",
    "Each image is associated with a label from 10 classes:\n",
    "\n",
    "| **Label** | **Description** |\n",
    "|-------|-------------|\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "`Fashion MNIST` was created as a more challenging drop-in replacement for the original MNIST dataset and shares the same image size, structure, and train/test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894e7db",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Import Libraries**\n",
    "\n",
    "We'll use the following libraries for this data preparation process:\n",
    "\n",
    "- **TensorFlow**: To download and access the Fashion MNIST dataset\n",
    "- **NumPy**: For efficient array operations and compressed storage\n",
    "- **pandas**: To create and manage CSV files for Vertex AI Datasets\n",
    "- **PIL (Python Imaging Library)**: For image processing and conversion\n",
    "- **json**: To create metadata files and class mapping\n",
    "- **os**: For filesystem operations\n",
    "- **datetime**: For tracking creation timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80607399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 14:52:35.816835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745765555.827071   71150 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745765555.830115   71150 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745765555.838817   71150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745765555.838833   71150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745765555.838835   71150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745765555.838836   71150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-27 14:52:35.841849: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f516f3",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Download and Prepare Data for Multiple Formats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a804de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local directory structure\n",
    "os.makedirs('./fashion_mnist_data', exist_ok=True)\n",
    "os.makedirs('./fashion_mnist_data/custom_jobs', exist_ok=True)\n",
    "os.makedirs('./fashion_mnist_data/vertex_datasets', exist_ok=True)\n",
    "os.makedirs('./fashion_mnist_data/vertex_datasets/train', exist_ok=True)\n",
    "os.makedirs('./fashion_mnist_data/vertex_datasets/test', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88949a",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Data Format Strategy**\n",
    "\n",
    "We're preparing the Fashion MNIST dataset in two distinct formats to support different ML training approaches in Google Cloud:\n",
    "\n",
    "**1. NumPy Compressed Format (for Custom Training Jobs):**\n",
    "This format stores all data in a single compressed NPZ file containing:\n",
    "- Training images (**X_train**)\n",
    "- Training labels (**y_train**)\n",
    "- Test images (**X_test**)\n",
    "- Test labels (**y_test**)\n",
    "\n",
    "Benefits:\n",
    "- `Efficient` storage and fast loading in `custom training` scripts\n",
    "- Reduced preprocessing overhead during training\n",
    "- Simple integration with `TensorFlow` and PyTorch custom `models`\n",
    "\n",
    "'''\n",
    "\n",
    "**2. Individual Images with CSV (for Vertex AI Datasets):**\n",
    "This format provides:\n",
    "- Individual `JPEG` images organized by `class`\n",
    "- `CSV` files with GCS paths and `labels` for import\n",
    "\n",
    "Benefits:\n",
    "- Direct import into `Vertex AI Datasets`\n",
    "- Compatible with `AutoML` training jobs\n",
    "- Supports visualization in the GCP console\n",
    "- Enables dataset versioning and management\n",
    "\n",
    "This dual-format approach provides flexibility for both `AutoML experimentation` and `custom model development`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b77a73aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28)\n",
      "Test data shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load Fashion MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Define class names\n",
    "class_names = ['T-shirt_top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle_boot']\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4bd0b",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Save in Multiple Formats**\n",
    "\n",
    "Each format is designed to optimize for different training workflows. \n",
    "\n",
    "Below we prepare both formats with proper organization and documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c8ba4",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    "#### **Format 1: NumPy Arrays (Best for Vertex AI Custom Jobs)**\n",
    "\n",
    "The `NumPy` compressed format provides an efficient storage solution for `custom training jobs`. The `.npz` file contains all necessary arrays, and the companion `class_names.json` file maps numeric labels to human-readable class names.\n",
    "\n",
    "This format is ideal for:\n",
    "- `Vertex AI` Custom Training Jobs\n",
    "- Custom Python training scripts\n",
    "- Local development and experimentation\n",
    "\n",
    "The compressed format significantly reduces storage requirements while maintaining fast load times during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2601902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy format for Custom Jobs saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Save as NumPy arrays for Custom Jobs\n",
    "# This format is most efficient for custom training scripts\n",
    "np.savez_compressed(\n",
    "    './fashion_mnist_data/custom_jobs/fashion_mnist.npz',\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Save class names\n",
    "with open('./fashion_mnist_data/custom_jobs/class_names.json', 'w') as f:\n",
    "    json.dump(class_names, f)\n",
    "\n",
    "print(\"NumPy format for Custom Jobs saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf46953",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    "#### **Format 2: Images with CSV (Best for Vertex AI Datasets)**\n",
    "\n",
    "The Vertex AI Datasets format organizes images into `class-specific folders` with accompanying CSV files for import. This format follows `GCP best practices` for image classification datasets.\n",
    "\n",
    "Key aspects:\n",
    "- Images are converted to `JPEG` format for storage efficiency\n",
    "- Files are organized by class for easier management\n",
    "- Three `CSV` files are provided:\n",
    "  - `train.csv`: For training data only\n",
    "  - `test.csv`: For test data only\n",
    "  - `all_data.csv`: Combined dataset for custom splits\n",
    "\n",
    "Each `CSV` contains `GCS paths` that match the expected bucket structure after upload. This format enables direct import into Vertex AI Datasets for `AutoML` training or `managed dataset` creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e684e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save images and create CSV for Vertex AI Datasets\n",
    "def create_vertex_dataset_format(images, labels, split='train'):\n",
    "    csv_data = []\n",
    "    \n",
    "    # Create class folders\n",
    "    for class_name in class_names:\n",
    "        os.makedirs(f'./fashion_mnist_data/vertex_datasets/{split}/{class_name}', exist_ok=True)\n",
    "    \n",
    "    for idx, (image, label) in enumerate(zip(images, labels)):\n",
    "        class_name = class_names[label]\n",
    "        image_filename = f\"{split}_{idx:05d}.jpg\"\n",
    "        \n",
    "        # Save as JPEG (better compression for storage)\n",
    "        local_path = f'./fashion_mnist_data/vertex_datasets/{split}/{class_name}/{image_filename}'\n",
    "        Image.fromarray(image).convert('L').save(local_path, 'JPEG', quality=95)\n",
    "        \n",
    "        # GCS path format for Vertex AI Datasets\n",
    "        gcs_path = f'gs://fashion-mnist-datasets/vertex_datasets/{split}/{class_name}/{image_filename}'\n",
    "        \n",
    "        # Add to CSV data\n",
    "        csv_data.append({\n",
    "            'gcs_path': gcs_path,\n",
    "            'label': class_name\n",
    "        })\n",
    "        \n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Processed {idx} {split} images...\")\n",
    "    \n",
    "    return pd.DataFrame(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d4f6b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 train images...\n",
      "Processed 1000 train images...\n",
      "Processed 2000 train images...\n",
      "Processed 3000 train images...\n",
      "Processed 4000 train images...\n",
      "Processed 5000 train images...\n",
      "Processed 6000 train images...\n",
      "Processed 7000 train images...\n",
      "Processed 8000 train images...\n",
      "Processed 9000 train images...\n",
      "Processed 10000 train images...\n",
      "Processed 11000 train images...\n",
      "Processed 12000 train images...\n",
      "Processed 13000 train images...\n",
      "Processed 14000 train images...\n",
      "Processed 15000 train images...\n",
      "Processed 16000 train images...\n",
      "Processed 17000 train images...\n",
      "Processed 18000 train images...\n",
      "Processed 19000 train images...\n",
      "Processed 20000 train images...\n",
      "Processed 21000 train images...\n",
      "Processed 22000 train images...\n",
      "Processed 23000 train images...\n",
      "Processed 24000 train images...\n",
      "Processed 25000 train images...\n",
      "Processed 26000 train images...\n",
      "Processed 27000 train images...\n",
      "Processed 28000 train images...\n",
      "Processed 29000 train images...\n",
      "Processed 30000 train images...\n",
      "Processed 31000 train images...\n",
      "Processed 32000 train images...\n",
      "Processed 33000 train images...\n",
      "Processed 34000 train images...\n",
      "Processed 35000 train images...\n",
      "Processed 36000 train images...\n",
      "Processed 37000 train images...\n",
      "Processed 38000 train images...\n",
      "Processed 39000 train images...\n",
      "Processed 40000 train images...\n",
      "Processed 41000 train images...\n",
      "Processed 42000 train images...\n",
      "Processed 43000 train images...\n",
      "Processed 44000 train images...\n",
      "Processed 45000 train images...\n",
      "Processed 46000 train images...\n",
      "Processed 47000 train images...\n",
      "Processed 48000 train images...\n",
      "Processed 49000 train images...\n",
      "Processed 50000 train images...\n",
      "Processed 51000 train images...\n",
      "Processed 52000 train images...\n",
      "Processed 53000 train images...\n",
      "Processed 54000 train images...\n",
      "Processed 55000 train images...\n",
      "Processed 56000 train images...\n",
      "Processed 57000 train images...\n",
      "Processed 58000 train images...\n",
      "Processed 59000 train images...\n",
      "Processed 0 test images...\n",
      "Processed 1000 test images...\n",
      "Processed 2000 test images...\n",
      "Processed 3000 test images...\n",
      "Processed 4000 test images...\n",
      "Processed 5000 test images...\n",
      "Processed 6000 test images...\n",
      "Processed 7000 test images...\n",
      "Processed 8000 test images...\n",
      "Processed 9000 test images...\n",
      "Vertex AI Datasets format saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_df = create_vertex_dataset_format(X_train, y_train, 'train')\n",
    "test_df = create_vertex_dataset_format(X_test, y_test, 'test')\n",
    "\n",
    "# Save CSV for Vertex AI import\n",
    "train_df.to_csv('./fashion_mnist_data/vertex_datasets/train.csv', index=False, header=False)\n",
    "test_df.to_csv('./fashion_mnist_data/vertex_datasets/test.csv', index=False, header=False)\n",
    "\n",
    "# Create combined CSV if needed\n",
    "combined_df = pd.concat([train_df, test_df])\n",
    "combined_df.to_csv('./fashion_mnist_data/vertex_datasets/all_data.csv', index=False, header=False)\n",
    "\n",
    "print(\"Vertex AI Datasets format saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdee209",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Documentation and Metadata**\n",
    "\n",
    "Proper documentation is critical for ML projects. Here we create:\n",
    "\n",
    "1. A comprehensive `README` file explaining the dataset structure and usage\n",
    "2. A manifest `JSON` file with dataset metadata for programmatic access\n",
    "\n",
    "These files ensure the dataset remains usable and well-documented for future reference and collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95925712",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    "#### **Create README and Manifest**\n",
    "\n",
    "The `README` provides `human-readable` `documentation` of the dataset structure, contents, and usage instructions. It includes `examples` for loading data in different contexts and serves as a reference for team members.\n",
    "\n",
    "The manifest `JSON` file provides `machine-readable metadata` that can be used by `automated processes` for dataset versioning, validation, and tracking. This approach follows ML engineering `best practices` for dataset management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "162b8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create README file\n",
    "readme_content = f\"\"\"# Fashion MNIST Dataset for Vertex AI\n",
    "\n",
    "This directory contains the Fashion MNIST dataset optimized for Google Cloud Vertex AI.\n",
    "\n",
    "## Directory Structure:\n",
    "- `/custom_jobs`: NumPy compressed arrays for Vertex AI Custom Training Jobs\n",
    "  - `fashion_mnist.npz`: Contains X_train, y_train, X_test, y_test\n",
    "  - `class_names.json`: List of class names\n",
    "  \n",
    "- `/vertex_datasets`: Images with CSV for Vertex AI Datasets\n",
    "  - `/train`: Training images organized by class\n",
    "  - `/test`: Test images organized by class\n",
    "  - `train.csv`: CSV for training data import\n",
    "  - `test.csv`: CSV for test data import\n",
    "  - `all_data.csv`: Combined CSV for full dataset import\n",
    "\n",
    "## Dataset Details:\n",
    "- Training samples: {len(X_train)}\n",
    "- Test samples: {len(X_test)}\n",
    "- Image dimensions: 28x28 grayscale\n",
    "- Classes: {', '.join(class_names)}\n",
    "\n",
    "## Usage:\n",
    "1. For Vertex AI Custom Training: Load the NPZ file\n",
    "   ```python\n",
    "   data = np.load('fashion_mnist.npz')\n",
    "   X_train, y_train = data['X_train'], data['y_train']\n",
    "   X_test, y_test = data['X_test'], data['y_test']\n",
    "2. For Vertex AI Datasets: Upload the CSV files to create Image Datasets\n",
    "\n",
    "Use all_data.csv for the complete dataset\n",
    "Use separate train/test CSVs for split datasets\n",
    "\n",
    "Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "with open('./fashion_mnist_data/README.md', 'w') as f:\n",
    "  f.write(readme_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82ef3dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README and manifest created successfully\n"
     ]
    }
   ],
   "source": [
    "manifest = {\n",
    "'dataset_name': 'Fashion MNIST',\n",
    "'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "'formats': {\n",
    "'custom_jobs': {\n",
    "'type': 'numpy_compressed',\n",
    "'files': ['fashion_mnist.npz', 'class_names.json']\n",
    "},\n",
    "'vertex_datasets': {\n",
    "'type': 'images_with_csv',\n",
    "'files': ['train.csv', 'test.csv', 'all_data.csv'],\n",
    "'image_format': 'JPEG'\n",
    "}\n",
    "},\n",
    "'statistics': {\n",
    "'train_samples': len(X_train),\n",
    "'test_samples': len(X_test),\n",
    "'image_shape': list(X_train[0].shape),\n",
    "'classes': class_names\n",
    "},\n",
    "'gcs_bucket': 'fashion-mnist-datasets'\n",
    "}\n",
    "with open('./fashion_mnist_data/manifest.json', 'w') as f:\n",
    "    json.dump(manifest, f, indent=4)\n",
    "\n",
    "print(\"README and manifest created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69a992",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Validation and Verification**\n",
    "\n",
    "Before uploading to `Google Cloud Storage`, we perform comprehensive validation checks to ensure:\n",
    "1. All `expected files` are present\n",
    "2. File `sizes` are reasonable\n",
    "3. `Image counts` match expected values\n",
    "4. Data structures contain the correct `shapes and types`\n",
    "\n",
    "This verification step prevents issues during cloud upload and subsequent ML workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480075e",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    "#### **Verify Local Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97e90961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying local files...\n",
      "✅ ./fashion_mnist_data/README.md (0.00 MB)\n",
      "✅ ./fashion_mnist_data/manifest.json (0.00 MB)\n",
      "✅ ./fashion_mnist_data/custom_jobs/fashion_mnist.npz (29.44 MB)\n",
      "✅ ./fashion_mnist_data/custom_jobs/class_names.json (0.00 MB)\n",
      "✅ ./fashion_mnist_data/vertex_datasets/train.csv (4.65 MB)\n",
      "✅ ./fashion_mnist_data/vertex_datasets/test.csv (0.76 MB)\n",
      "✅ ./fashion_mnist_data/vertex_datasets/all_data.csv (5.40 MB)\n",
      "\n",
      "Image counts:\n",
      "Training images: 60000\n",
      "Test images: 10000\n",
      "\n",
      "NPZ file contents:\n",
      "- X_train: shape (60000, 28, 28)\n",
      "- y_train: shape (60000,)\n",
      "- X_test: shape (10000, 28, 28)\n",
      "- y_test: shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Verify local files\n",
    "def verify_local_files():\n",
    "    print(\"\\nVerifying local files...\")\n",
    "    \n",
    "    files_to_check = [\n",
    "        './fashion_mnist_data/README.md',\n",
    "        './fashion_mnist_data/manifest.json',\n",
    "        './fashion_mnist_data/custom_jobs/fashion_mnist.npz',\n",
    "        './fashion_mnist_data/custom_jobs/class_names.json',\n",
    "        './fashion_mnist_data/vertex_datasets/train.csv',\n",
    "        './fashion_mnist_data/vertex_datasets/test.csv',\n",
    "        './fashion_mnist_data/vertex_datasets/all_data.csv'\n",
    "    ]\n",
    "    \n",
    "    for file_path in files_to_check:\n",
    "        if os.path.exists(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"✅ {file_path} ({size_mb:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"❌ {file_path} missing\")\n",
    "    \n",
    "    # Count images\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        train_path = f'./fashion_mnist_data/vertex_datasets/train/{class_name}'\n",
    "        test_path = f'./fashion_mnist_data/vertex_datasets/test/{class_name}'\n",
    "        \n",
    "        if os.path.exists(train_path):\n",
    "            train_count += len([f for f in os.listdir(train_path) if f.endswith('.jpg')])\n",
    "        if os.path.exists(test_path):\n",
    "            test_count += len([f for f in os.listdir(test_path) if f.endswith('.jpg')])\n",
    "    \n",
    "    print(f\"\\nImage counts:\")\n",
    "    print(f\"Training images: {train_count}\")\n",
    "    print(f\"Test images: {test_count}\")\n",
    "    \n",
    "    # Verify NPZ file\n",
    "    try:\n",
    "        data = np.load('./fashion_mnist_data/custom_jobs/fashion_mnist.npz')\n",
    "        print(f\"\\nNPZ file contents:\")\n",
    "        for key in data.files:\n",
    "            print(f\"- {key}: shape {data[key].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading NPZ file: {e}\")\n",
    "\n",
    "verify_local_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d85dfe",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "With the dataset properly formatted, documented, and verified, the next steps are:\n",
    "\n",
    "1. **Upload to Google Cloud Storage**: `gsutil -m cp -r ./fashion_mnist_data/* gs://fashion-mnist-datasets/`\n",
    "\n",
    "2. **Create Vertex AI Dataset**:\n",
    "- Using the GCP Console \n",
    "- Import the `all_data.csv` or separate `train/test CSVs`\n",
    "- Set appropriate dataset type (`image classification`)\n",
    "\n",
    "3. **Initialize Custom Training or AutoML**:\n",
    "- For `AutoML`: Use the created Vertex AI Dataset\n",
    "- For `Custom` Training: Reference the NPZ file in your training code\n",
    "\n",
    "This completes the data preparation phase for the Fashion MNIST project, establishing a foundation for model development on Google Cloud Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76df1a9",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "This notebook successfully prepared the `Fashion MNIST dataset` in `multiple formats` optimized for Google Cloud Platform's machine learning services. The preparation includes:\n",
    "\n",
    "- **Data organization** in both NPZ and individual image formats\n",
    "- **Comprehensive documentation** through README and manifest files\n",
    "- **Verification of data integrity** through automated checks\n",
    "- **Preparation for cloud upload** with properly structured paths\n",
    "\n",
    "The resulting `dataset` is ready for both AutoML training and custom model development, providing a flexible foundation for the `Fashion MNIST classification project`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9097228e",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4141589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow-GPU)",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
