{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad94507",
   "metadata": {},
   "source": [
    "# **Fashion MNIST: Raw Data Analysis**\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c53cc08",
   "metadata": {},
   "source": [
    "### **Import Libraries and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5af97cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 10:00:15.026358: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745748015.038239   17397 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745748015.041383   17397 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745748015.050863   17397 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745748015.050875   17397 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745748015.050877   17397 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745748015.050878   17397 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-27 10:00:15.054862: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import cv2\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f173ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for analysis results\n",
    "os.makedirs('./analysis_results', exist_ok=True)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d5aae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28)\n",
      "Test data shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load Fashion MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Define class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc959a5",
   "metadata": {},
   "source": [
    "### **1. Dataset Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420601c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dataset Overview\n",
    "def analyze_dataset_overview():\n",
    "    # Class distribution\n",
    "    train_dist = np.bincount(y_train)\n",
    "    test_dist = np.bincount(y_test)\n",
    "    \n",
    "    # Create distribution visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    ax1.bar(class_names, train_dist)\n",
    "    ax1.set_title('Training Set Class Distribution')\n",
    "    ax1.set_xlabel('Class')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    ax2.bar(class_names, test_dist)\n",
    "    ax2.set_title('Test Set Class Distribution')\n",
    "    ax2.set_xlabel('Class')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./analysis_results/class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save distribution data\n",
    "    distribution_data = {\n",
    "        'train_distribution': dict(zip(class_names, train_dist.tolist())),\n",
    "        'test_distribution': dict(zip(class_names, test_dist.tolist())),\n",
    "        'image_shape': X_train[0].shape,\n",
    "        'total_train_samples': len(X_train),\n",
    "        'total_test_samples': len(X_test)\n",
    "    }\n",
    "    \n",
    "    with open('./analysis_results/dataset_overview.json', 'w') as f:\n",
    "        json.dump(distribution_data, f, indent=4)\n",
    "    \n",
    "    return distribution_data\n",
    "\n",
    "overview_data = analyze_dataset_overview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e42ffd5",
   "metadata": {},
   "source": [
    "### **2. Statistical Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb5d185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17397/2598978855.py:35: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(brightness_data, labels=class_names)\n"
     ]
    }
   ],
   "source": [
    "# 2. Statistical Analysis\n",
    "def statistical_analysis():\n",
    "    # Global statistics\n",
    "    global_mean = np.mean(X_train)\n",
    "    global_std = np.std(X_train)\n",
    "    \n",
    "    # Class-specific statistics\n",
    "    class_stats = {}\n",
    "    for i in range(10):\n",
    "        class_data = X_train[y_train == i]\n",
    "        class_stats[class_names[i]] = {\n",
    "            'mean': float(np.mean(class_data)),\n",
    "            'std': float(np.std(class_data)),\n",
    "            'min': float(np.min(class_data)),\n",
    "            'max': float(np.max(class_data)),\n",
    "            'median': float(np.median(class_data))\n",
    "        }\n",
    "    \n",
    "    # Pixel intensity distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(X_train.flatten(), bins=50, alpha=0.7)\n",
    "    plt.title('Pixel Intensity Distribution')\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig('./analysis_results/pixel_intensity_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Class-wise brightness analysis\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    brightness_data = []\n",
    "    for i in range(10):\n",
    "        class_data = X_train[y_train == i]\n",
    "        brightness_data.append(np.mean(class_data, axis=(1, 2)))\n",
    "    \n",
    "    ax.boxplot(brightness_data, labels=class_names)\n",
    "    ax.set_title('Class-wise Brightness Distribution')\n",
    "    ax.set_ylabel('Mean Brightness')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./analysis_results/class_brightness_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save statistical data\n",
    "    stats_data = {\n",
    "        'global_statistics': {\n",
    "            'mean': float(global_mean),\n",
    "            'std': float(global_std)\n",
    "        },\n",
    "        'class_statistics': class_stats\n",
    "    }\n",
    "    \n",
    "    with open('./analysis_results/statistical_analysis.json', 'w') as f:\n",
    "        json.dump(stats_data, f, indent=4)\n",
    "    \n",
    "    return stats_data\n",
    "\n",
    "stats_data = statistical_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df36496",
   "metadata": {},
   "source": [
    "### **3. Visual Pattern Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d18f1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Visual Pattern Analysis\n",
    "def visual_pattern_analysis():\n",
    "    # Sample images for each class\n",
    "    fig, axes = plt.subplots(10, 10, figsize=(15, 15))\n",
    "    for i in range(10):\n",
    "        class_indices = np.where(y_train == i)[0][:10]\n",
    "        for j in range(10):\n",
    "            axes[i, j].imshow(X_train[class_indices[j]], cmap='gray')\n",
    "            axes[i, j].axis('off')\n",
    "            if j == 0:\n",
    "                axes[i, j].set_ylabel(class_names[i], rotation=90, size='large')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./analysis_results/class_samples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Edge detection analysis\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    for i in range(10):\n",
    "        class_example = X_train[y_train == i][0]\n",
    "        edges = cv2.Canny(class_example, 100, 200)\n",
    "        \n",
    "        ax = axes[i // 5, i % 5]\n",
    "        ax.imshow(edges, cmap='gray')\n",
    "        ax.set_title(class_names[i])\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./analysis_results/edge_detection_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Average image per class\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    average_images = []\n",
    "    for i in range(10):\n",
    "        class_images = X_train[y_train == i]\n",
    "        avg_img = np.mean(class_images, axis=0)\n",
    "        average_images.append(avg_img)\n",
    "        \n",
    "        ax = axes[i // 5, i % 5]\n",
    "        ax.imshow(avg_img, cmap='gray')\n",
    "        ax.set_title(f'Average {class_names[i]}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./analysis_results/average_class_images.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return average_images\n",
    "\n",
    "average_images = visual_pattern_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361f820",
   "metadata": {},
   "source": [
    "### **4. Dimensionality Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a64b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dimensionality Analysis\n",
    "def dimensionality_analysis():\n",
    "    # Reshape data for analysis\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    \n",
    "    # PCA Analysis\n",
    "    pca = PCA(n_components=50)\n",
    "    X_pca = pca.fit_transform(X_train_flat)\n",
    "    \n",
    "    # Plot explained variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.title('PCA: Explained Variance vs Components')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('./analysis_results/pca_explained_variance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # t-SNE visualization (using a subset for efficiency)\n",
    "    np.random.seed(42)\n",
    "    subset_indices = np.random.choice(len(X_train_flat), 5000, replace=False)\n",
    "    X_subset = X_train_flat[subset_indices]\n",
    "    y_subset = y_train[subset_indices]\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_subset)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_subset, cmap='tab10', alpha=0.6)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('t-SNE Visualization of Fashion MNIST')\n",
    "    \n",
    "    # Add class labels\n",
    "    for i in range(10):\n",
    "        plt.annotate(class_names[i], \n",
    "                    xy=(np.mean(X_tsne[y_subset == i, 0]), \n",
    "                        np.mean(X_tsne[y_subset == i, 1])),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', fc='yellow', alpha=0.7))\n",
    "    \n",
    "    plt.savefig('./analysis_results/tsne_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save dimensionality data\n",
    "    dim_data = {\n",
    "        'pca_explained_variance_ratio': pca.explained_variance_ratio_.tolist(),\n",
    "        'pca_components_needed_95': int(np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1)\n",
    "    }\n",
    "    \n",
    "    with open('./analysis_results/dimensionality_analysis.json', 'w') as f:\n",
    "        json.dump(dim_data, f, indent=4)\n",
    "    \n",
    "    return dim_data\n",
    "\n",
    "dim_data = dimensionality_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c563f",
   "metadata": {},
   "source": [
    "### **5. Class Relationship Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be8576d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Class Relationship Analysis\n",
    "def class_relationship_analysis():\n",
    "    # Calculate class centroids\n",
    "    centroids = []\n",
    "    for i in range(10):\n",
    "        class_images = X_train[y_train == i]\n",
    "        centroid = np.mean(class_images, axis=0)\n",
    "        centroids.append(centroid.flatten())\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = cosine_similarity(centroids)\n",
    "    \n",
    "    # Plot similarity heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, annot=True, cmap='coolwarm', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Class Similarity Matrix (Cosine Similarity)')\n",
    "    plt.savefig('./analysis_results/class_similarity_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Identify most similar class pairs\n",
    "    similar_pairs = []\n",
    "    for i in range(10):\n",
    "        for j in range(i + 1, 10):\n",
    "            similar_pairs.append({\n",
    "                'class1': class_names[i],\n",
    "                'class2': class_names[j],\n",
    "                'similarity': float(similarity_matrix[i, j])\n",
    "            })\n",
    "    \n",
    "    similar_pairs.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    # Save relationship data\n",
    "    relationship_data = {\n",
    "        'similarity_matrix': similarity_matrix.tolist(),\n",
    "        'most_similar_pairs': similar_pairs[:5],\n",
    "        'least_similar_pairs': similar_pairs[-5:]\n",
    "    }\n",
    "    \n",
    "    with open('./analysis_results/class_relationships.json', 'w') as f:\n",
    "        json.dump(relationship_data, f, indent=4)\n",
    "    \n",
    "    return relationship_data\n",
    "\n",
    "relationship_data = class_relationship_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f58a8",
   "metadata": {},
   "source": [
    "### **6. Data Quality Assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f124393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Data Quality Assessment\n",
    "def data_quality_assessment():\n",
    "    # Check for outliers using z-score\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    mean_intensity = np.mean(X_train_flat, axis=1)\n",
    "    z_scores = (mean_intensity - np.mean(mean_intensity)) / np.std(mean_intensity)\n",
    "    \n",
    "    outlier_indices = np.where(np.abs(z_scores) > 3)[0]\n",
    "    \n",
    "    # Plot potential outliers\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.ravel()\n",
    "    for i, idx in enumerate(outlier_indices[:10]):\n",
    "        axes[i].imshow(X_train[idx], cmap='gray')\n",
    "        axes[i].set_title(f'Potential Outlier\\n{class_names[y_train[idx]]}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./analysis_results/potential_outliers.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Check data consistency\n",
    "    quality_data = {\n",
    "        'outlier_count': len(outlier_indices),\n",
    "        'outlier_percentage': float(len(outlier_indices) / len(X_train) * 100),\n",
    "        'data_range': {\n",
    "            'min': float(np.min(X_train)),\n",
    "            'max': float(np.max(X_train))\n",
    "        },\n",
    "        'null_values': bool(np.isnan(X_train).any()),\n",
    "        'data_type': str(X_train.dtype)\n",
    "    }\n",
    "    \n",
    "    with open('./analysis_results/data_quality_assessment.json', 'w') as f:\n",
    "        json.dump(quality_data, f, indent=4)\n",
    "    \n",
    "    return quality_data\n",
    "\n",
    "quality_data = data_quality_assessment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e39fb",
   "metadata": {},
   "source": [
    "### **7. Preprocessing Insights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df5a68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Preprocessing Insights\n",
    "def preprocessing_insights():\n",
    "    # Compare normalization techniques\n",
    "    sample_image = X_train[0]\n",
    "    \n",
    "    # Different normalization methods\n",
    "    normalizations = {\n",
    "        'Original': sample_image,\n",
    "        'Min-Max [0,1]': sample_image / 255.0,\n",
    "        'Mean Normalization': (sample_image - np.mean(sample_image)) / np.std(sample_image),\n",
    "        'Standard Scaling': (sample_image - np.mean(X_train)) / np.std(X_train)\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (name, img) in enumerate(normalizations.items()):\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(name)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./analysis_results/normalization_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Analyze brightness and contrast effects\n",
    "    augmentation_examples = {\n",
    "        'Original': sample_image,\n",
    "        'Brightness +50': np.clip(sample_image + 50, 0, 255),\n",
    "        'Brightness -50': np.clip(sample_image - 50, 0, 255),\n",
    "        'Contrast x1.5': np.clip(sample_image * 1.5, 0, 255)\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (name, img) in enumerate(augmentation_examples.items()):\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(name)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./analysis_results/augmentation_examples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    preprocessing_recommendations = {\n",
    "        'recommended_normalization': 'Min-Max [0,1]',\n",
    "        'reason': 'Maintains relative pixel intensities while scaling to standard range',\n",
    "        'augmentation_suggestions': [\n",
    "            'Random rotation (±10 degrees)',\n",
    "            'Random horizontal flip',\n",
    "            'Random brightness adjustment (±10%)',\n",
    "            'Random zoom (0.9-1.1x)'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open('./analysis_results/preprocessing_insights.json', 'w') as f:\n",
    "        json.dump(preprocessing_recommendations, f, indent=4)\n",
    "    \n",
    "    return preprocessing_recommendations\n",
    "\n",
    "preprocessing_data = preprocessing_insights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa0220",
   "metadata": {},
   "source": [
    "### **8. Performance Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c235461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Performance Predictions\n",
    "def performance_predictions():\n",
    "    # Based on analysis, predict difficulty of classification\n",
    "    class_difficulty = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        class_images = X_train[y_train == i]\n",
    "        \n",
    "        # Calculate intra-class variance\n",
    "        variance = np.mean(np.var(class_images, axis=0))\n",
    "        \n",
    "        # Get similarity to other classes\n",
    "        similar_classes = []\n",
    "        for j in range(10):\n",
    "            if i != j:\n",
    "                similarity = relationship_data['similarity_matrix'][i][j]\n",
    "                similar_classes.append((class_names[j], similarity))\n",
    "        \n",
    "        similar_classes.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        class_difficulty.append({\n",
    "            'class': class_names[i],\n",
    "            'variance': float(variance),\n",
    "            'most_similar_to': similar_classes[0][0],\n",
    "            'similarity_score': float(similar_classes[0][1])\n",
    "        })\n",
    "    \n",
    "    # Sort by difficulty (higher variance and similarity = more difficult)\n",
    "    class_difficulty.sort(key=lambda x: x['variance'] * x['similarity_score'], reverse=True)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    classes = [item['class'] for item in class_difficulty]\n",
    "    difficulty_scores = [item['variance'] * item['similarity_score'] for item in class_difficulty]\n",
    "    \n",
    "    bars = ax.bar(classes, difficulty_scores)\n",
    "    ax.set_ylabel('Difficulty Score')\n",
    "    ax.set_title('Predicted Classification Difficulty by Class')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./analysis_results/classification_difficulty_prediction.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    performance_data = {\n",
    "        'class_difficulty_ranking': class_difficulty,\n",
    "        'expected_confusion_pairs': [\n",
    "            {'pair': [item['class'], item['most_similar_to']], \n",
    "             'similarity': item['similarity_score']} \n",
    "            for item in class_difficulty[:5]\n",
    "        ],\n",
    "        'estimated_accuracy_range': {\n",
    "            'simple_model': '85-90%',\n",
    "            'complex_model': '92-95%',\n",
    "            'ensemble': '94-97%'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('./analysis_results/performance_predictions.json', 'w') as f:\n",
    "        json.dump(performance_data, f, indent=4)\n",
    "    \n",
    "    return performance_data\n",
    "\n",
    "performance_data = performance_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dcf3e3",
   "metadata": {},
   "source": [
    "### **9. Comparative Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ad84e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# 9. Comparative Analysis with Original MNIST\n",
    "def comparative_analysis():\n",
    "    # Load original MNIST for comparison\n",
    "    from tensorflow.keras.datasets import mnist\n",
    "    (mnist_train, _), _ = mnist.load_data()\n",
    "    \n",
    "    # Compare complexity\n",
    "    fashion_complexity = np.mean(np.std(X_train, axis=(1, 2)))\n",
    "    mnist_complexity = np.mean(np.std(mnist_train, axis=(1, 2)))\n",
    "    \n",
    "    # Compare edge density\n",
    "    fashion_edges = np.mean([np.mean(cv2.Canny(img, 100, 200) > 0) for img in X_train[:1000]])\n",
    "    mnist_edges = np.mean([np.mean(cv2.Canny(img, 100, 200) > 0) for img in mnist_train[:1000]])\n",
    "    \n",
    "    # Visual comparison\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(15, 4))\n",
    "    \n",
    "    for i in range(10):\n",
    "        axes[0, i].imshow(X_train[i], cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, 0].set_ylabel('Fashion\\nMNIST', rotation=0, labelpad=40)\n",
    "        \n",
    "        axes[1, i].imshow(mnist_train[i], cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, 0].set_ylabel('Original\\nMNIST', rotation=0, labelpad=40)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./analysis_results/mnist_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    comparison_data = {\n",
    "        'complexity_comparison': {\n",
    "            'fashion_mnist': float(fashion_complexity),\n",
    "            'original_mnist': float(mnist_complexity),\n",
    "            'complexity_ratio': float(fashion_complexity / mnist_complexity)\n",
    "        },\n",
    "        'edge_density_comparison': {\n",
    "            'fashion_mnist': float(fashion_edges),\n",
    "            'original_mnist': float(mnist_edges),\n",
    "            'edge_ratio': float(fashion_edges / mnist_edges)\n",
    "        },\n",
    "        'key_differences': [\n",
    "            'Fashion MNIST has more complex patterns and textures',\n",
    "            'Higher intra-class variability in Fashion MNIST',\n",
    "            'More challenging due to visual similarity between classes',\n",
    "            'Requires more sophisticated feature extraction'\n",
    "        ],\n",
    "        'architecture_recommendations': [\n",
    "            'Use deeper CNNs for Fashion MNIST',\n",
    "            'Consider attention mechanisms',\n",
    "            'Implement data augmentation',\n",
    "            'Use ensemble methods for best performance'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open('./analysis_results/comparative_analysis.json', 'w') as f:\n",
    "        json.dump(comparison_data, f, indent=4)\n",
    "    \n",
    "    return comparison_data\n",
    "\n",
    "comparison_data = comparative_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd3038",
   "metadata": {},
   "source": [
    "### **Executive Summary Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed712a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete! All results saved to ./analysis_results/\n"
     ]
    }
   ],
   "source": [
    "# Generate Executive Summary\n",
    "def generate_executive_summary():\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    summary = {\n",
    "        'analysis_timestamp': timestamp,\n",
    "        'dataset_summary': {\n",
    "            'total_samples': overview_data['total_train_samples'] + overview_data['total_test_samples'],\n",
    "            'image_dimensions': overview_data['image_shape'],\n",
    "            'number_of_classes': 10,\n",
    "            'class_balance': 'Balanced (6,000 samples per class)'\n",
    "        },\n",
    "        'key_findings': {\n",
    "            'data_quality': f\"{quality_data['outlier_percentage']:.2f}% potential outliers detected\",\n",
    "            'dimensionality': f\"{dim_data['pca_components_needed_95']} components explain 95% variance\",\n",
    "            'class_relationships': f\"Most similar classes: {relationship_data['most_similar_pairs'][0]['class1']} and {relationship_data['most_similar_pairs'][0]['class2']}\",\n",
    "            'complexity': f\"{comparison_data['complexity_comparison']['complexity_ratio']:.2f}x more complex than original MNIST\"\n",
    "        },\n",
    "        'recommendations': {\n",
    "            'preprocessing': preprocessing_data['recommended_normalization'],\n",
    "            'architecture': comparison_data['architecture_recommendations'][0],\n",
    "            'expected_performance': performance_data['estimated_accuracy_range']['complex_model']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('./analysis_results/executive_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "    \n",
    "    # Create a visual summary\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Class distribution\n",
    "    train_dist = np.bincount(y_train)\n",
    "    ax1.bar(class_names, train_dist)\n",
    "    ax1.set_title('Class Distribution')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # PCA variance\n",
    "    pca_variance = dim_data['pca_explained_variance_ratio'][:20]\n",
    "    ax2.plot(range(1, 21), np.cumsum(pca_variance))\n",
    "    ax2.set_title('PCA Explained Variance')\n",
    "    ax2.set_xlabel('Components')\n",
    "    ax2.set_ylabel('Cumulative Variance')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Class similarity\n",
    "    im = ax3.imshow(relationship_data['similarity_matrix'], cmap='coolwarm')\n",
    "    ax3.set_xticks(range(10))\n",
    "    ax3.set_yticks(range(10))\n",
    "    ax3.set_xticklabels(class_names, rotation=45)\n",
    "    ax3.set_yticklabels(class_names)\n",
    "    ax3.set_title('Class Similarity Matrix')\n",
    "    plt.colorbar(im, ax=ax3)\n",
    "    \n",
    "    # Difficulty prediction\n",
    "    classes = [item['class'] for item in performance_data['class_difficulty_ranking']][:5]\n",
    "    scores = [item['variance'] * item['similarity_score'] for item in performance_data['class_difficulty_ranking']][:5]\n",
    "    ax4.bar(classes, scores)\n",
    "    ax4.set_title('Top 5 Most Difficult Classes')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./analysis_results/executive_summary_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Analysis complete! All results saved to ./analysis_results/\")\n",
    "    return summary\n",
    "\n",
    "executive_summary = generate_executive_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9601e225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fashion MNIST Analysis...\n",
      "1. Dataset Overview...\n",
      "2. Statistical Analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17397/2598978855.py:35: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(brightness_data, labels=class_names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Visual Pattern Analysis...\n",
      "4. Dimensionality Analysis...\n",
      "5. Class Relationship Analysis...\n",
      "6. Data Quality Assessment...\n",
      "7. Preprocessing Insights...\n",
      "8. Performance Predictions...\n",
      "9. Comparative Analysis...\n",
      "10. Generating Executive Summary...\n",
      "Analysis complete! All results saved to ./analysis_results/\n",
      "\n",
      "Analysis Complete! Check ./analysis_results/ for all outputs.\n"
     ]
    }
   ],
   "source": [
    "def run_complete_analysis():\n",
    "    print(\"Starting Fashion MNIST Analysis...\")\n",
    "    \n",
    "    print(\"1. Dataset Overview...\")\n",
    "    analyze_dataset_overview()\n",
    "    \n",
    "    print(\"2. Statistical Analysis...\")\n",
    "    statistical_analysis()\n",
    "    \n",
    "    print(\"3. Visual Pattern Analysis...\")\n",
    "    visual_pattern_analysis()\n",
    "    \n",
    "    print(\"4. Dimensionality Analysis...\")\n",
    "    dimensionality_analysis()\n",
    "    \n",
    "    print(\"5. Class Relationship Analysis...\")\n",
    "    class_relationship_analysis()\n",
    "    \n",
    "    print(\"6. Data Quality Assessment...\")\n",
    "    data_quality_assessment()\n",
    "    \n",
    "    print(\"7. Preprocessing Insights...\")\n",
    "    preprocessing_insights()\n",
    "    \n",
    "    print(\"8. Performance Predictions...\")\n",
    "    performance_predictions()\n",
    "    \n",
    "    print(\"9. Comparative Analysis...\")\n",
    "    comparative_analysis()\n",
    "    \n",
    "    print(\"10. Generating Executive Summary...\")\n",
    "    generate_executive_summary()\n",
    "    \n",
    "    print(\"\\nAnalysis Complete! Check ./analysis_results/ for all outputs.\")\n",
    "\n",
    "# Run the complete analysis\n",
    "if __name__ == \"__main__\":\n",
    "    run_complete_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557393e3",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded1714f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow-GPU)",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
